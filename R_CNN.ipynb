{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CWIHz1URWERbE4_bz1BRdsLbbHQI5N8k","timestamp":1676595675784}],"authorship_tag":"ABX9TyMKn+sgWHGjUBYd50cyBb1y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["jupyter-lab --allow-root --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0"],"metadata":{"id":"1vjHbyq9PhaV"}},{"cell_type":"markdown","source":["Test R-CNN Code (Tensorflow)"],"metadata":{"id":"mEi2oytLJ-ln"}},{"cell_type":"markdown","source":["1. Create training data\n","Generate Region Proposals\n","Region proposals are generated using the selective search algorithm. Selective search is an algorithm that groups similar elements based texture, color, shape or size. It is hierachical, so it starts to define very fine-grained elements and combines them to bigger elements. Ultimately it will yield multiple region proposals. It is noteworthy that this algorithm has nothing to do with machine learning, later methods replace that element with neural networks.\n","\n","Check region proposals\n","Since we have the real object detection rectangles for an image, we can compare the region proposal to the ground truths. We use a method called intersection over union (IoU). It measures the overlap between the predicted proposal and ground truth. If it is bigger than 50%, we classify the prediction as a positiv sample. In this way we build our positive and negative training classes.\n","\n","Extract images\n","In this implementation we crop the images (using the region proposals) and save them as images in a folder."],"metadata":{"id":"i2f7iYJLM57B"}},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","import pathlib\n","import xml.etree.ElementTree as ET\n","import matplotlib.pyplot as plt\n","import random\n","import numpy as np\n","import os\n","from glob import glob\n","from tqdm import tqdm\n","import requests\n","import cv2\n","from PIL import Image"],"metadata":{"id":"9GkeCysKMaez","executionInfo":{"status":"ok","timestamp":1688631218859,"user_tz":-540,"elapsed":218,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def extract_ground_truth(path):\n","    \"\"\"Extracts ground truths like boxes, labels and ids from xml files.\"\"\"\n","\n","    boxes = []\n","    labels = []\n","    ids = []\n","\n","    tree = ET.parse(path)\n","    id_i = path.split(\"/\")[-1].split(\".\")[1]\n","    objects = tree.getroot().findall(\"object\")\n","\n","    for object_i in objects:\n","        object_name = object_i.find(\"name\").text\n","        if object_name==\"cat\":\n","            instances = object_i.findall(\"bndbox\")\n","            for instance in instances:\n","                xmin = int(float(instance.find(\"xmin\").text))\n","                xmax = int(float(instance.find(\"xmax\").text))\n","                ymin = int(float(instance.find(\"ymin\").text))\n","                ymax = int(float(instance.find(\"ymax\").text))\n","                box = np.array([xmin, xmax, ymin, ymax])\n","                boxes.append(box)\n","                labels.append(object_name)\n","                ids.append(id_i)\n","        return np.array(boxes), np.array(labels), np.array(ids) #We will only consider last item, can be changed in future iterations"],"metadata":{"id":"dxKa8UplMa8-","executionInfo":{"status":"ok","timestamp":1688631222335,"user_tz":-540,"elapsed":7,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def region_proposals(image):\n","    \"\"\"Generates region proposals using Selective Search\"\"\"\n","    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","    ss.setBaseImage(image)\n","    ss.switchToSelectiveSearchFast()\n","    rects = ss.process()\n","\n","    prediction_rects = _restructure_rectangles_xx(rects)\n","    return prediction_rects\n","\n","def _restructure_rectangles_xx(old_rects):\n","    \"\"\"Converts rectangles from (x, y, w, h) to (x1, x2, y1, y2)\"\"\"\n","    x1 = old_rects[:,0]\n","    x2 = old_rects[:,2]+old_rects[:,0]\n","    y1 = old_rects[:,1]\n","    y2 = old_rects[:,1]+old_rects[:,3]\n","\n","    rects_new = old_rects.copy()\n","    rects_new[:,0] = x1\n","    rects_new[:,1] = x2\n","    rects_new[:,2] = y1\n","    rects_new[:,3] = y2\n","    return rects_new"],"metadata":{"id":"UdFYqN3VMgkY","executionInfo":{"status":"ok","timestamp":1688631223915,"user_tz":-540,"elapsed":3,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def images_to_file(image, boxes, labels, predicted_rectangles, image_id, path=\".\"):\n","    \"\"\"Crops boxes and writes images to files.\"\"\"\n","\n","    if not os.path.exists(f\"{path}/{labels[0]}\"):\n","        os.makedirs(f\"{path}/{labels[0]}\")\n","    if not os.path.exists(f\"{path}/no_category\"):\n","        os.makedirs(f\"{path}/no_category\")\n","\n","    # Iterate through boxes\n","    for box, label, i_id in zip(boxes, labels, image_id):\n","        i_pos = i_neg = 0\n","        for rect in predicted_rectangles:\n","            iou = _get_iou(box,rect)\n","            if iou > 0.5:\n","                i_pos +=1\n","                file_name = f\"{path}/{label}/{i_id}_{i_pos}.jpg\"\n","                cropped_image = image[rect[2]:rect[3], rect[0]:rect[1]]\n","                img = Image.fromarray(cropped_image, 'RGB')\n","                img2 = img.resize((227,227), Image.ANTIALIAS)\n","                img2.save(file_name)\n","            elif iou < 0.1 and i_neg < 50:\n","                if (rect[3]-rect[2])>20 and (rect[1]-rect[0])>20:\n","                    i_neg += 1\n","                    file_name = f\"{path}/no_category/{label}_{i_id}_{i_neg}.jpg\"\n","                    cropped_image = image[rect[2]:rect[3], rect[0]:rect[1]]\n","                    img = Image.fromarray(cropped_image, 'RGB')\n","                    img2 = img.resize((227,227), Image.ANTIALIAS)\n","                    img2.save(file_name)\n"],"metadata":{"id":"Wt1wxEIuMiX_","executionInfo":{"status":"ok","timestamp":1688631225568,"user_tz":-540,"elapsed":5,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Inspired by https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n","def _get_iou(bb1, bb2):\n","    \"\"\"\n","    Calculate the Intersection over Union (IoU) of two bounding boxes.\n","\n","    Parameters\n","    ----------\n","    bb1 : array\n","        Keys: {'x1', 'x2', 'y1', 'y2'}\n","        The (x1, y1) position is at the top left corner,\n","        the (x2, y2) position is at the bottom right corner\n","    bb2 : array\n","        Keys: {'x1', 'x2', 'y1', 'y2'}\n","        The (x, y) position is at the top left corner,\n","        the (x2, y2) position is at the bottom right corner\n","\n","    Returns\n","    -------\n","    float\n","        in [0, 1]\n","    \"\"\"\n","    assert bb1[0] < bb1[1]\n","    assert bb1[2] < bb1[3]\n","    assert bb2[0] < bb2[1]\n","    assert bb2[2] < bb2[3]\n","\n","    # determine the coordinates of the intersection rectangle\n","    x_left = max(bb1[0], bb2[0])\n","    y_top = max(bb1[2], bb2[2])\n","    x_right = min(bb1[1], bb2[1])\n","    y_bottom = min(bb1[3], bb2[3])\n","\n","    if x_right < x_left or y_bottom < y_top:\n","        return 0.0\n","\n","    # The intersection of two axis-aligned bounding boxes is always an\n","    # axis-aligned bounding box\n","    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","\n","    # compute the area of both AABBs\n","    bb1_area = (bb1[1] - bb1[0]) * (bb1[3] - bb1[2])\n","    bb2_area = (bb2[1] - bb2[0]) * (bb2[3] - bb2[2])\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n","    assert iou >= 0.0\n","    assert iou <= 1.0\n","    return iou"],"metadata":{"id":"iLXl_dIMMlVU","executionInfo":{"status":"ok","timestamp":1688631228136,"user_tz":-540,"elapsed":6,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["metadata_paths = sorted(glob(\"../input/asirra-cats-vs-dogs-object-detection-dataset/Asirra: cat vs dogs/cat*.xml\"))\n","images_paths = sorted(glob(\"../input/asirra-cats-vs-dogs-object-detection-dataset/Asirra: cat vs dogs/cat*.jpg\"))"],"metadata":{"id":"1X1SI7cqMtvW","executionInfo":{"status":"ok","timestamp":1688631236763,"user_tz":-540,"elapsed":4,"user":{"displayName":"신성한","userId":"02352508022902564974"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["for metadata_path, image_path in tqdm(zip(metadata_paths, images_paths)):\n","    if os.stat(image_path).st_size < 100000:\n","        image = cv2.imread(image_path)\n","\n","        truth_recangles, truth_label, image_id = extract_ground_truth(metadata_path)\n","\n","        if truth_label:\n","            predicted_rectangles = region_proposals(image)\n","            images_to_file(image, truth_recangles, truth_label, predicted_rectangles, image_id, \"./training\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sx45uqoBMu1a","executionInfo":{"status":"ok","timestamp":1688631240072,"user_tz":-540,"elapsed":28,"user":{"displayName":"신성한","userId":"02352508022902564974"}},"outputId":"5cb864af-4135-4f08-f559-1144e14b438e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]}]},{"cell_type":"markdown","source":["2. Train Neural Network\n","In the original paper the authors use a neural network to output a 4096-dimensional representation of an image. This implementations classifies the output (region proposals of cat vs. no cat) directly. It uses transfer learning by re-training a MobileNetV2. Futhermore various steps to augment the image data by shifting, rotating or flipping pixels are used.\n","\n","I simplified the training (no verification data, no hyperparameter search) to save some of my time ;)"],"metadata":{"id":"drLE9GPxMy6E"}},{"cell_type":"code","source":["data_train = keras.utils.image_dataset_from_directory(\"/Users/crossrunway/DataSets/dogs-vs-cats/train\", image_size=(227,227))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"id":"Zd4o_G6OM7Ws","executionInfo":{"status":"error","timestamp":1688631548561,"user_tz":-540,"elapsed":691,"user":{"displayName":"신성한","userId":"02352508022902564974"}},"outputId":"5ced055f-24b2-4416-9c6e-78a1d6155e53"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 0 files belonging to 0 classes.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/crossrunway/DataSets/dogs-vs-cats/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m227\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m227\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ObjectDetection/lib/python3.11/site-packages/keras/src/utils/image_dataset.py:299\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[1;32m    296\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    304\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[1;32m    305\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mimage_paths,\n\u001b[1;32m    306\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     crop_to_aspect_ratio\u001b[38;5;241m=\u001b[39mcrop_to_aspect_ratio,\n\u001b[1;32m    313\u001b[0m )\n\u001b[1;32m    314\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n","\u001b[0;31mValueError\u001b[0m: No images found in directory /Users/crossrunway/DataSets/dogs-vs-cats/train. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"]}]},{"cell_type":"code","source":["data_train.class_names"],"metadata":{"id":"ScRgXlZvM-JW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspired by https://pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/\n","# Create neural network\n","class_weight = {0: 5, 1: 1}\n","\n","input_l = keras.layers.Input(shape=(227, 227, 3))\n","input_layer = keras.layers.Rescaling(1./255)(input_l)\n","input_layer = keras.layers.ZeroPadding2D((16,16))(input_layer)\n","input_layer = keras.layers.RandomCrop(227,227)(input_layer)\n","input_layer = keras.layers.RandomFlip(\"horizontal\")(input_layer)\n","\n","headModel = keras.applications.MobileNetV2(weights=\"imagenet\", include_top=False,input_tensor=input_layer)\n","\n","for layer in headModel.layers:\n","    layer.trainable = False\n","\n","headModel = headModel.output\n","\n","baseModel = keras.layers.AveragePooling2D(pool_size=(7, 7))(headModel)\n","baseModel = keras.layers.Flatten(name=\"flatten\")(baseModel)\n","baseModel = keras.layers.Dense(128, activation=\"relu\")(baseModel)\n","baseModel = keras.layers.Dropout(0.3)(baseModel)\n","output = keras.layers.Dense(2, activation=\"softmax\")(baseModel)\n","\n","\n","model = keras.models.Model(inputs=input_l, outputs=output)"],"metadata":{"id":"ofFytnUDNPyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n","\n","history = model.fit(data_train, epochs=10, batch_size=64, class_weight=class_weight)"],"metadata":{"id":"OaNRJhHlNSYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"5tczVJQ8NUnH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Prediction\n","Download prediction data\n","For demonstration purpose I use an example image from Google Images search to verify the algorithm."],"metadata":{"id":"fTEoqHMvNatw"}},{"cell_type":"code","source":["prediction_sample = \"https://i.ytimg.com/vi/K1Xkt1E7PJY/maxresdefault.jpg\"\n","\n","response = requests.get(prediction_sample)\n","\n","with open(\"./prediction.jpg\", 'wb') as f:\n","    f.write(response.content)\n","\n","prediction = plt.imread(\"./prediction.jpg\")\n","\n","plt.imshow(prediction)\n","plt.show()"],"metadata":{"id":"wQzBsp3yNbRL"},"execution_count":null,"outputs":[]}]}