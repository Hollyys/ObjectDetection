{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CWIHz1URWERbE4_bz1BRdsLbbHQI5N8k","timestamp":1676595675784}],"authorship_tag":"ABX9TyONOZpuYX1pToMEL2FW5ZJj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Test R-CNN Code (Tensorflow)"],"metadata":{"id":"mEi2oytLJ-ln"}},{"cell_type":"markdown","source":["1. Create training data\n","Generate Region Proposals\n","Region proposals are generated using the selective search algorithm. Selective search is an algorithm that groups similar elements based texture, color, shape or size. It is hierachical, so it starts to define very fine-grained elements and combines them to bigger elements. Ultimately it will yield multiple region proposals. It is noteworthy that this algorithm has nothing to do with machine learning, later methods replace that element with neural networks.\n","\n","Check region proposals\n","Since we have the real object detection rectangles for an image, we can compare the region proposal to the ground truths. We use a method called intersection over union (IoU). It measures the overlap between the predicted proposal and ground truth. If it is bigger than 50%, we classify the prediction as a positiv sample. In this way we build our positive and negative training classes.\n","\n","Extract images\n","In this implementation we crop the images (using the region proposals) and save them as images in a folder."],"metadata":{"id":"i2f7iYJLM57B"}},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","import pathlib\n","import xml.etree.ElementTree as ET\n","import matplotlib.pyplot as plt\n","import random\n","import numpy as np\n","import os\n","from glob import glob\n","from tqdm import tqdm\n","import requests\n","import cv2\n","from PIL import Image"],"metadata":{"id":"9GkeCysKMaez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_ground_truth(path):\n","    \"\"\"Extracts ground truths like boxes, labels and ids from xml files.\"\"\"\n","\n","    boxes = []\n","    labels = []\n","    ids = []\n","\n","    tree = ET.parse(path)\n","    id_i = path.split(\"/\")[-1].split(\".\")[1]\n","    objects = tree.getroot().findall(\"object\")\n","\n","    for object_i in objects:\n","        object_name = object_i.find(\"name\").text\n","        if object_name==\"cat\":\n","            instances = object_i.findall(\"bndbox\")\n","            for instance in instances:\n","                xmin = int(float(instance.find(\"xmin\").text))\n","                xmax = int(float(instance.find(\"xmax\").text))\n","                ymin = int(float(instance.find(\"ymin\").text))\n","                ymax = int(float(instance.find(\"ymax\").text))\n","                box = np.array([xmin, xmax, ymin, ymax])\n","                boxes.append(box)\n","                labels.append(object_name)\n","                ids.append(id_i)\n","        return np.array(boxes), np.array(labels), np.array(ids) #We will only consider last item, can be changed in future iterations"],"metadata":{"id":"dxKa8UplMa8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def region_proposals(image):\n","    \"\"\"Generates region proposals using Selective Search\"\"\"\n","    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n","    ss.setBaseImage(image)\n","    ss.switchToSelectiveSearchFast()\n","    rects = ss.process()\n","\n","    prediction_rects = _restructure_rectangles_xx(rects)\n","    return prediction_rects\n","\n","def _restructure_rectangles_xx(old_rects):\n","    \"\"\"Converts rectangles from (x, y, w, h) to (x1, x2, y1, y2)\"\"\"\n","    x1 = old_rects[:,0]\n","    x2 = old_rects[:,2]+old_rects[:,0]\n","    y1 = old_rects[:,1]\n","    y2 = old_rects[:,1]+old_rects[:,3]\n","\n","    rects_new = old_rects.copy()\n","    rects_new[:,0] = x1\n","    rects_new[:,1] = x2\n","    rects_new[:,2] = y1\n","    rects_new[:,3] = y2\n","    return rects_new"],"metadata":{"id":"UdFYqN3VMgkY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def images_to_file(image, boxes, labels, predicted_rectangles, image_id, path=\".\"):\n","    \"\"\"Crops boxes and writes images to files.\"\"\"\n","\n","    if not os.path.exists(f\"{path}/{labels[0]}\"):\n","        os.makedirs(f\"{path}/{labels[0]}\")\n","    if not os.path.exists(f\"{path}/no_category\"):\n","        os.makedirs(f\"{path}/no_category\")\n","\n","    # Iterate through boxes\n","    for box, label, i_id in zip(boxes, labels, image_id):\n","        i_pos = i_neg = 0\n","        for rect in predicted_rectangles:\n","            iou = _get_iou(box,rect)\n","            if iou > 0.5:\n","                i_pos +=1\n","                file_name = f\"{path}/{label}/{i_id}_{i_pos}.jpg\"\n","                cropped_image = image[rect[2]:rect[3], rect[0]:rect[1]]\n","                img = Image.fromarray(cropped_image, 'RGB')\n","                img2 = img.resize((227,227), Image.ANTIALIAS)\n","                img2.save(file_name)\n","            elif iou < 0.1 and i_neg < 50:\n","                if (rect[3]-rect[2])>20 and (rect[1]-rect[0])>20:\n","                    i_neg += 1\n","                    file_name = f\"{path}/no_category/{label}_{i_id}_{i_neg}.jpg\"\n","                    cropped_image = image[rect[2]:rect[3], rect[0]:rect[1]]\n","                    img = Image.fromarray(cropped_image, 'RGB')\n","                    img2 = img.resize((227,227), Image.ANTIALIAS)\n","                    img2.save(file_name)\n"],"metadata":{"id":"Wt1wxEIuMiX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspired by https://stackoverflow.com/questions/25349178/calculating-percentage-of-bounding-box-overlap-for-image-detector-evaluation\n","def _get_iou(bb1, bb2):\n","    \"\"\"\n","    Calculate the Intersection over Union (IoU) of two bounding boxes.\n","\n","    Parameters\n","    ----------\n","    bb1 : array\n","        Keys: {'x1', 'x2', 'y1', 'y2'}\n","        The (x1, y1) position is at the top left corner,\n","        the (x2, y2) position is at the bottom right corner\n","    bb2 : array\n","        Keys: {'x1', 'x2', 'y1', 'y2'}\n","        The (x, y) position is at the top left corner,\n","        the (x2, y2) position is at the bottom right corner\n","\n","    Returns\n","    -------\n","    float\n","        in [0, 1]\n","    \"\"\"\n","    assert bb1[0] < bb1[1]\n","    assert bb1[2] < bb1[3]\n","    assert bb2[0] < bb2[1]\n","    assert bb2[2] < bb2[3]\n","\n","    # determine the coordinates of the intersection rectangle\n","    x_left = max(bb1[0], bb2[0])\n","    y_top = max(bb1[2], bb2[2])\n","    x_right = min(bb1[1], bb2[1])\n","    y_bottom = min(bb1[3], bb2[3])\n","\n","    if x_right < x_left or y_bottom < y_top:\n","        return 0.0\n","\n","    # The intersection of two axis-aligned bounding boxes is always an\n","    # axis-aligned bounding box\n","    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","\n","    # compute the area of both AABBs\n","    bb1_area = (bb1[1] - bb1[0]) * (bb1[3] - bb1[2])\n","    bb2_area = (bb2[1] - bb2[0]) * (bb2[3] - bb2[2])\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n","    assert iou >= 0.0\n","    assert iou <= 1.0\n","    return iou"],"metadata":{"id":"iLXl_dIMMlVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metadata_paths = sorted(glob(\"../input/asirra-cats-vs-dogs-object-detection-dataset/Asirra: cat vs dogs/cat*.xml\"))\n","images_paths = sorted(glob(\"../input/asirra-cats-vs-dogs-object-detection-dataset/Asirra: cat vs dogs/cat*.jpg\"))"],"metadata":{"id":"1X1SI7cqMtvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for metadata_path, image_path in tqdm(zip(metadata_paths, images_paths)):\n","    if os.stat(image_path).st_size < 100000:\n","        image = cv2.imread(image_path)\n","\n","        truth_recangles, truth_label, image_id = extract_ground_truth(metadata_path)\n","\n","        if truth_label:\n","            predicted_rectangles = region_proposals(image)\n","            images_to_file(image, truth_recangles, truth_label, predicted_rectangles, image_id, \"./training\")"],"metadata":{"id":"sx45uqoBMu1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Train Neural Network\n","In the original paper the authors use a neural network to output a 4096-dimensional representation of an image. This implementations classifies the output (region proposals of cat vs. no cat) directly. It uses transfer learning by re-training a MobileNetV2. Futhermore various steps to augment the image data by shifting, rotating or flipping pixels are used.\n","\n","I simplified the training (no verification data, no hyperparameter search) to save some of my time ;)"],"metadata":{"id":"drLE9GPxMy6E"}},{"cell_type":"code","source":["data_train = keras.utils.image_dataset_from_directory(\"./training\", image_size=(227,227))"],"metadata":{"id":"Zd4o_G6OM7Ws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train.class_names"],"metadata":{"id":"ScRgXlZvM-JW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspired by https://pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/\n","# Create neural network\n","class_weight = {0: 5, 1: 1}\n","\n","input_l = keras.layers.Input(shape=(227, 227, 3))\n","input_layer = keras.layers.Rescaling(1./255)(input_l)\n","input_layer = keras.layers.ZeroPadding2D((16,16))(input_layer)\n","input_layer = keras.layers.RandomCrop(227,227)(input_layer)\n","input_layer = keras.layers.RandomFlip(\"horizontal\")(input_layer)\n","\n","headModel = keras.applications.MobileNetV2(weights=\"imagenet\", include_top=False,input_tensor=input_layer)\n","\n","for layer in headModel.layers:\n","    layer.trainable = False\n","\n","headModel = headModel.output\n","\n","baseModel = keras.layers.AveragePooling2D(pool_size=(7, 7))(headModel)\n","baseModel = keras.layers.Flatten(name=\"flatten\")(baseModel)\n","baseModel = keras.layers.Dense(128, activation=\"relu\")(baseModel)\n","baseModel = keras.layers.Dropout(0.3)(baseModel)\n","output = keras.layers.Dense(2, activation=\"softmax\")(baseModel)\n","\n","\n","model = keras.models.Model(inputs=input_l, outputs=output)"],"metadata":{"id":"ofFytnUDNPyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n","\n","history = model.fit(data_train, epochs=10, batch_size=64, class_weight=class_weight)"],"metadata":{"id":"OaNRJhHlNSYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"5tczVJQ8NUnH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Prediction\n","Download prediction data\n","For demonstration purpose I use an example image from Google Images search to verify the algorithm."],"metadata":{"id":"fTEoqHMvNatw"}},{"cell_type":"code","source":["prediction_sample = \"https://i.ytimg.com/vi/K1Xkt1E7PJY/maxresdefault.jpg\"\n","\n","response = requests.get(prediction_sample)\n","\n","with open(\"./prediction.jpg\", 'wb') as f:\n","    f.write(response.content)\n","\n","prediction = plt.imread(\"./prediction.jpg\")\n","\n","plt.imshow(prediction)\n","plt.show()"],"metadata":{"id":"wQzBsp3yNbRL"},"execution_count":null,"outputs":[]}]}